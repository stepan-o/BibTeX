Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Wolpert1996,
abstract = {This is the first of two papers that use off-training set {\{}(OTS){\}} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no {\{}$\backslash$textbackslash{\}}textita priori distinctions between learning algorithms. {\{}(The{\}} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected {\{}OTS{\}} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the {\{}Vapnik-Chervonenkis{\}} dimension of your generalizer is small; and the training set is large, then with high probability your {\{}OTS{\}} error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
author = {Wolpert, David H},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Wolpert(1996) - The Lack of A Priori Distinctions Between Learning Algorithms.pdf:pdf},
journal = {Neural Computation},
number = {7},
pages = {1391--1420},
title = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
volume = {8},
year = {1996}
}
@article{McCulloch1990a,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed,},
author = {McCulloch, Warren S and Pitts, Walter},
doi = {Doi 10.1016/S0092-8240(05)80006-0},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/1990{\_}-{\_}Warren{\_}McCulloch{\_}-{\_}Alogicalcalculusoftheideasimmanentinnervousactivit[retrieved{\_}2019-04-06].pdf:pdf},
isbn = {0092-8240},
issn = {0092-8240},
journal = {Bulletin of Mathematical Biology},
number = {1--2},
pages = {99--115},
title = {{A logical calculus of the ideas immanent in nervous activity (reprinted from bulletin of mathematical biophysics, vol 5, pg 115-133, 1943)}},
url = {http://journals2.scholarsportal.info/pdf/00928240/v52i1-2/99{\_}alcotiiina.xml},
volume = {52},
year = {1990}
}
@techreport{Rosenblatt1957a,
address = {Buffalo, NY},
author = {Rosenblatt, Frank},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/rosenblatt-1957.pdf:pdf},
institution = {Cornell Aeronautical Laboratory, Inc.},
title = {{The Perceptron: a Percieving and Recognizing Automation}},
url = {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
year = {1957}
}
@techreport{Widrow1960,
abstract = {A new circuit element called a "memistor" (a resistor with memory) has been devised that will have general use in adaptive circuits. With such an element it is possible to get an electronically variable gain control along with the memory required for storage of the system's experiences or training. Experiences are stored in their most compact form, and in a form that is directly usable from the standpoint of system functioning. The element consists of a resistive graphite substrate immersed in a plating bath. The resistance is reversibly controlled by elecroplating. The memistror element has been applied to the realization of adaptive neurons. Memistor circuits for the "Adaline" neuron, which inporporate its simple adaption procedure, have been developed. It has been possible to train these neurons so that this training will remain effective for weeks. Steps have been taken toward the minituarization of the memistor element. The memistor promises a cheap, reliable, mass-producible, adaptive-system element.},
address = {Stanford},
author = {Widrow, Bernanrd and Hoff, M E},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Widrow(1960) - An Adaptive Adaline Neuron Using Chemical Memistors.pdf:pdf},
institution = {Stanford University},
title = {{An Adaptive "Adaline" Neuron Using Chemical "Memistors", Technical Report No. 1553-2}},
url = {http://www-isl.stanford.edu/{~}widrow/papers/t1960anadaptive.pdf},
year = {1960}
}
