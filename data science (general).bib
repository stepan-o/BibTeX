Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lazer2017,
abstract = {Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a number of nascent but important trends in the use of big data.},
author = {Lazer, David and Radford, J.},
doi = {10.1146/annurev-soc-060116-053457},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/2017{\_}-{\_}David{\_}Lazer{\_}-{\_}DataexMachinaIntroductiontoBigData[retrieved{\_}2018-10-26].pdf:pdf},
isbn = {978-0-8243-2243-4},
issn = {0360-0572},
journal = {Ssrn},
pages = {19--39},
title = {{Data Ex Machina: Introduction to Big Data}},
year = {2017}
}
@article{McCulloch1990a,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed,},
author = {McCulloch, Warren S and Pitts, Walter},
doi = {Doi 10.1016/S0092-8240(05)80006-0},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/1990{\_}-{\_}Warren{\_}McCulloch{\_}-{\_}Alogicalcalculusoftheideasimmanentinnervousactivit[retrieved{\_}2019-04-06].pdf:pdf},
isbn = {0092-8240},
issn = {0092-8240},
journal = {Bulletin of Mathematical Biology},
number = {1--2},
pages = {99--115},
title = {{A logical calculus of the ideas immanent in nervous activity (reprinted from bulletin of mathematical biophysics, vol 5, pg 115-133, 1943)}},
url = {http://journals2.scholarsportal.info/pdf/00928240/v52i1-2/99{\_}alcotiiina.xml},
volume = {52},
year = {1990}
}
@article{Inuwa-dutse2018,
abstract = {a b s t r a c t Online Social Media platforms, such as Facebook and Twitter, enable all users, independently of their characteristics, to freely generate and consume huge amounts of data. While this data is being exploited by individuals and organisations to gain competitive advantage, a substantial amount of data is being generated by spam or fake users. One in every 200 social media messages and one in every 21 tweets is estimated to be spam. The rapid growth in the volume of global spam is expected to compromise research works that use social media data, thereby questioning data credibility. Motivated by the need to identify and filter out spam contents in social media data, this study presents a novel approach for distinguishing spam vs. non-spam social media posts and offers more insight into the behaviour of spam users on Twitter. The approach proposes an optimised set of features independent of historical tweets, which are only available for a short time on Twitter. We take into account features related to the users of Twitter, their accounts and their pairwise engagement with each other. We experimentally demonstrate the efficacy and robustness of our approach and compare it to a typical feature set for spam detection in the literature, achieving a significant improvement on performance. In contrast to prior research findings, we observe that an average automated spam account posted at least 12 tweets per day at well defined periods. Our method is suitable for real-time deployment in a social media data collection pipeline as an initial preprocessing strategy to improve the validity of research data. {\textcopyright}},
author = {Inuwa-dutse, Isa and Liptrott, Mark and Korkontzelos, Ioannis},
doi = {10.1016/j.neucom.2018.07.044},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Detection{\_}of{\_}spam{\_}twitter.pdf:pdf},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Social media,Social network,Spam,Spam detection,Twitter,Twitter microblog},
pages = {496--511},
publisher = {Elsevier B.V.},
title = {{Detection of spam-posting accounts on Twitter}},
url = {https://doi.org/10.1016/j.neucom.2018.07.044},
volume = {315},
year = {2018}
}
@article{Nisar2018,
abstract = {In order to explore the relationship between politics-related sentiment and FTSE 100 movements, we conducted a short-window event study of a UK based political event. We collected a sample of over 60,000 tweets using 3 key hashtags during the period of 6 days including before, during and after the 2016 local elections. The study involved performing a collection of correlation and regression analyses to compare daily mood with daily changes in the price of the FTSE 100 at the market level. The findings suggest that there is evidence of correlation between the general mood of the public and investment behavior in the short term; however, the relationship is not yet determined as statistically significant. There is also evidence of causation between public sentiment and the stock market movements, in terms of the relationship between MOOD and the daily closing price, and the time lag findings of MOOD and PRICE. Overall, these results show promise for using sentiment analytics on Twitter data for forecasting market movements.},
author = {Nisar, Tahir M. and Yeung, Man},
doi = {10.1016/j.jfds.2017.11.002},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/1-s2.0-S2405918817300247-main.pdf:pdf},
issn = {24059188},
journal = {The Journal of Finance and Data Science},
keywords = {ftse 100,political discussions,sentiment analysis,stock market prediction,twitter},
number = {2},
pages = {101--119},
publisher = {Elsevier Ltd},
title = {{Twitter as a tool for forecasting stock market movements: AÂ short-window event study}},
url = {https://doi.org/10.1016/j.jfds.2017.11.002},
volume = {4},
year = {2018}
}
@article{Hilbert2011,
author = {Hilbert, Martin and Lopez, Priscila},
doi = {10.1126/science.1200970},
file = {:C$\backslash$:/Users/HP USER/Documents/Python Scripts/MIE1628 Big Data Science/Papers/hilbert2011a.pdf:pdf},
journal = {Science},
number = {332},
pages = {60--65},
title = {{The World's Technological Capacity to Store, Communicate, and Compute Information}},
url = {http://www.uvm.edu/pdodds/files/papers/others/2011/hilbert2011a.pdf},
volume = {60},
year = {2011}
}
@book{Codd1990,
address = {Boston, MA},
author = {Codd, Edgar F},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/codd-relational{\_}model{\_}db{\_}management.pdf:pdf},
isbn = {ISBN:0-201-14192-2},
publisher = {Addison-Wesley Longman Publishing Co.},
title = {{The relational model for database management : version 2}},
year = {1990}
}
@article{Batty2012,
abstract = {Here we sketch the rudiments of what constitutes a smart city which we define as a city in which ICT is merged with traditional infrastructures, coordinated and integrated using new digital technologies. We first sketch our vision defining seven goals which concern: developing a new understanding of urban problems; effective and feasible ways to coordinate urban technologies; models and methods for using urban data across spatial and temporal scales; developing new technologies for communication and dissemination; developing new forms of urban governance and organisation; defining critical problems relating to cities, transport, and energy; and identifying risk, uncertainty, and hazards in the smart city. To this, we add six research challenges: to relate the infrastructure of smart cities to their operational functioning and planning through management, control and optimisation; to explore the notion of the city as a laboratory for innovation; to provide portfolios of urban simulation which inform future designs; to develop technologies that ensure equity, fairness and realise a better quality of city life; to develop technologies that ensure informed participation and create shared knowledge for democratic city governance; and to ensure greater and more effective mobility and access to opportunities for urban populations. We begin by defining the state of the art, explaining the science of smart cities. We define six scenarios based on new cities badging themselves as smart, older cities regenerating themselves as smart, the development of science parks, tech cities, and technopoles focused on high technologies, the development of urban services using contemporary ICT, the use of ICT to develop new urban intelligence functions, and the development of online and mobile forms of participation. Seven project areas are then proposed: Integrated Databases for the Smart City, Sensing, Networking and the Impact of New Social Media, Modelling Network Performance, Mobility and Travel Behaviour, Modelling Urban Land Use, Transport and Economic Interactions, Modelling Urban Transactional Activities in Labour and Housing Markets, Decision Support as Urban Intelligence, Participatory Governance and Planning Structures for the Smart City. Finally we anticipate the paradigm shifts that will occur in this research and define a series of key demonstrators which we believe are important to progressing a science of smart cities.},
archivePrefix = {arXiv},
arxivId = {0706.0024},
author = {Batty, M. and Axhausen, K. W. and Giannotti, F. and Pozdnoukhov, A. and Bazzani, A. and Wachowicz, M. and Ouzounis, G. and Portugali, Y.},
doi = {10.1140/epjst/e2012-01703-3},
eprint = {0706.0024},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/2012{\_}-{\_}M{\_}Batty{\_}-{\_}Smartcitiesofthefuture[retrieved{\_}2018-10-28].pdf:pdf},
isbn = {1951-6355},
issn = {19516355},
journal = {European Physical Journal: Special Topics},
number = {1},
pages = {481--518},
pmid = {18764023},
title = {{Smart cities of the future}},
volume = {214},
year = {2012}
}
@techreport{Rosenblatt1957a,
address = {Buffalo, NY},
author = {Rosenblatt, Frank},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/rosenblatt-1957.pdf:pdf},
institution = {Cornell Aeronautical Laboratory, Inc.},
title = {{The Perceptron: a Percieving and Recognizing Automation}},
url = {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
year = {1957}
}
@article{Lazer2014,
author = {Lazer, David and Kennedy, Ryan and King, Gary and Vespignani, Alessandro},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/The Parable of Google Flu.pdf:pdf},
journal = {Science},
number = {March},
pages = {1203--1205},
title = {{The Parable of Google Flu: Traps in Big Data Analysis}},
volume = {343},
year = {2014}
}
@article{Krizhevsky2007,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called âdropoutâ that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1201/9781420010749},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781420010749},
journal = {Handbook of Approximation Algorithms and Metaheuristics},
pages = {60--1--60--16},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2007}
}
@article{DAgostino1971a,
abstract = {We present a test of normality based on a statistic D which is up to a constant the ratio of Downton's linear unbiased estimator of the population standard deviation to the sample standard deviation. For the usual levels of significance Monte Carlo simulations indicate that Cornish-Fisher expansions adequately approximate the null distribution of D if the sample size is 50 or more. The test is an omnibus test, being appropriate to detect deviations from normality due either to skewness or kurtosis. Simulation results of powers for various alternatives when the sample size is 50 indicate that the test compares favourably with the Shapiro-Wilk W test, Vbl, b2 and the ratio of range to stand},
author = {D'Agostino, Ralph and Rosman, Bernard},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/DAgostino(1971) - An Omnibus Test of Normality for Moderate and Large Size Samples.pdf:pdf},
journal = {Biometrika},
number = {58},
pages = {341--348},
title = {{An Omnibus Test of Normality for Moderate and Large Size Samples}},
url = {http://www.jstor.org/stable/2334302},
volume = {2},
year = {1971}
}
@article{Christ2016,
abstract = {The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm for time series, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features. The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics.},
archivePrefix = {arXiv},
arxivId = {1610.07717},
author = {Christ, Maximilian and Kempa-Liehr, Andreas W. and Feindt, Michael},
eprint = {1610.07717},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Distributed and parallel time series feature extraction for big data applications.pdf:pdf},
keywords = {feature engineering,feature selection,time series feature},
title = {{Distributed and parallel time series feature extraction for industrial big data applications}},
url = {http://arxiv.org/abs/1610.07717},
year = {2016}
}
@article{Bullynck2010,
author = {Bullynck, Maarten},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Johann Lambert's Scientific Tool Kit.pdf:pdf},
journal = {Science in Context, Cambridge University Press (CUP)},
number = {1},
pages = {pp.65--89},
title = {{Johann Lambert's Scientific Tool Kit: Exemplified by the Measurement of Humidity}},
volume = {23},
year = {2010}
}
@article{Wickham2011,
abstract = {Many data analysis problems involve the application of a split-apply-combine strategy, where you break up a big problem into manageable pieces, operate on each piece inde- pendently and then put all the pieces back together. This insight gives rise to a new R package that allows you to smoothly apply this strategy, without having to worry about the type of structure in which your data is stored. The paper includes two case studies showing how these insights make it easier to work with batting records for veteran baseball players and a large 3d array of spatio-temporal ozone measurements.},
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/H. Wickham (2011) -- The Split-Apply-Combine Strategy for Data Analysis.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {apply,data analysis,r,split},
number = {1},
title = {{The Split-Apply-Combine Strategy for Data}},
url = {https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf},
volume = {40},
year = {2011}
}
@article{Smith2009a,
author = {Smith, Tony E.},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/SPATIAL WEIGHT MATRICES.pdf:pdf},
journal = {Geographical Analysis},
title = {{Spatial weights matrices}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.2009.00768.x/full},
year = {2009}
}
@article{Arribas-Bel2014,
abstract = {In this paper, I review the recent emergence of three groups of data sources and assess some of the opportunities and challenges they pose for the understanding of cities, particularly in the context of the Regional Science and urban research agenda. These are data collected from mobile sensors carried by individuals, data derived from businesses moving their activity online and government data released in an open format. Although very different from each other, they are all becoming available as a side-effect since they were created with different purposes but their degree of popularity, pervasiveness and ease of access is turning them into interesting alternatives for researchers. Existing projects and initiatives that conform to each class are featured as illustrative examples of these new potential sources of knowledge. {\textcopyright} 2013 Elsevier Ltd.},
author = {Arribas-Bel, Daniel},
doi = {10.1016/j.apgeog.2013.09.012},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/D.Arribas-Bell - Accidental, open and everywhere - Emerging data sources for the.pdf:pdf},
isbn = {0143-6228},
issn = {01436228},
journal = {Applied Geography},
keywords = {Cities,Data sources,Open data},
pages = {45--53},
publisher = {Elsevier Ltd},
title = {{Accidental, open and everywhere: Emerging data sources for the understanding of cities}},
url = {http://dx.doi.org/10.1016/j.apgeog.2013.09.012},
volume = {49},
year = {2014}
}
@article{Wickham2014,
abstract = {In this paper we present the R package gRain for propagation in graphical indepen- dence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
eprint = {arXiv:1501.0228},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Tidy{\_}Data{\_}v59i10.pdf:pdf},
isbn = {9780387781662},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {data cleaning,data tidying,r,relational databases},
number = {10},
pmid = {18291371},
title = {{Tidy Data}},
url = {http://www.jstatsoft.org/v59/i10/},
volume = {59},
year = {2014}
}
